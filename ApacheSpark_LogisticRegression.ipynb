{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark-Mllib: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression, despite its name, is a linear model for classification rather than regression. \n",
    "Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or\n",
    "the log-linear classifier. \n",
    "In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Apache Spark </b> </h2>\n",
    "<p>Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in <b>Java, Scala, Python and R,</b> and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "\n",
    "\n",
    "PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system,\n",
    "HDFS, Cassandra, HBase, Amazon S3, etc. \n",
    "Spark supports text files, SequenceFiles, and any other Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data and creating the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initializing Spark</h3>\n",
    "<p>The first thing a Spark program must do is to create a SparkContext object, which tells Spark how to access a cluster.\n",
    "To create a SparkContext you first need to build a SparkConf object that contains information about your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext,SQLContext\n",
    "sc = SparkContext(\"local\",\"App:LogisticRegression\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training Dataset </h3>\n",
    "<p> Get the training data from web. You have data in the Data/KDD folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size is 4898431\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "#f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \"Data/kddcup.data.gz\")\n",
    "data_file = \"Data/KDD/kddcup.data.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "print \"Train data size is {}\".format(raw_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testing Dataset </h3>\n",
    "<p> Get the testing data from web  You have data in the Data/KDD folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size is 311029\n"
     ]
    }
   ],
   "source": [
    "#ft = urllib.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\", \"Data/KDD/corrected.gz\")\n",
    "test_data_file = \"Data/KDD/corrected.gz\"\n",
    "test_raw_data = sc.textFile(test_data_file)\n",
    "print \"Test data size is {}\".format(test_raw_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement \n",
    "<p>The task for the classifier learning was to learn a predictive model (i.e. a classifier) capable of distinguishing between legitimate and illegitimate connections in a computer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array\n",
    "\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "test_csv_data = test_raw_data.map(lambda x: x.split(\",\"))\n",
    "\n",
    "protocols = csv_data.map(lambda x: x[1]).distinct().collect()\n",
    "services = csv_data.map(lambda x: x[2]).distinct().collect()\n",
    "flags = csv_data.map(lambda x: x[3]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python lists to create_labeled_point function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def create_labeled_point(line_split):\n",
    "    # leave_out = [41]\n",
    "    clean_line_split = line_split[0:41]\n",
    "    # convert protocol to numeric categorical variable\n",
    "    try: \n",
    "        clean_line_split[1] = protocols.index(clean_line_split[1])\n",
    "    except:\n",
    "        clean_line_split[1] = len(protocols)\n",
    "        \n",
    "    # convert service to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[2] = services.index(clean_line_split[2])\n",
    "    except:\n",
    "        clean_line_split[2] = len(services)\n",
    "    \n",
    "    # convert flag to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[3] = flags.index(clean_line_split[3])\n",
    "    except:\n",
    "        clean_line_split[3] = len(flags)\n",
    "    \n",
    "    # convert label to binary label\n",
    "    attack = 1.0\n",
    "    if line_split[41]=='normal.':\n",
    "        attack = 0.0\n",
    "        \n",
    "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
    "\n",
    "training_data = csv_data.map(create_labeled_point)\n",
    "test_data = test_csv_data.map(create_labeled_point)\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training a classifier</h2>\n",
    "<p>We are now ready to train our classification tree.\n",
    "We will keep the maxDepth value small. \n",
    "This will lead to smaller accuracy, but we will obtain less splits so later on we can better interpret the tree.\n",
    "In a production system you can try to increase this value in order to find a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained in 505.15 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from time import time\n",
    "\n",
    "# Build the model\n",
    "t0 = time()\n",
    "logit_model = LogisticRegressionWithLBFGS.train(training_data)\n",
    "tt = time() - t0\n",
    "\n",
    "print \"Classifier trained in {} seconds\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to measure the classification error on our test data, we use map on the test_data RDD and the model to predict each test point class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction made in 19.977 seconds. Test accuracy is 0.9177\n"
     ]
    }
   ],
   "source": [
    "predictions = logit_model.predict(test_data.map(lambda p: p.features))\n",
    "predictionAndLabels = test_data.map(lambda p: p.label).zip(predictions)\n",
    "t0 = time()\n",
    "test_accuracy = predictionAndLabels.filter(lambda (v, p): v == p).count() / float(test_data.count())\n",
    "tt = time() - t0\n",
    "print \"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Interpreting the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients/Weights: [-7.61612481061e-05,0.572777754905,-0.028412343255,-0.208141050977,1.33412892262e-08,4.40458603763e-09,-1.5811380696,1.78079128595,0.267368242166,0.184485336008,1.26105749478,-1.68818890979,0.00194065274937,1.35159159005,-1.10522718864,-0.00252600728549,-0.123812803684,-0.736733537563,-0.728267027959,0.0,-2.81876106643,-2.41155806648,0.00622188381142,0.00390815684301,0.609569772261,0.602668329268,0.0950634368692,0.186583722948,-1.47036834056,-0.594097248532,-0.166081579422,0.0084799697183,-0.000521304771949,-0.240344061568,-0.954949460816,1.73342424282,4.09003837096,0.608292870568,0.611242984004,0.216019075778,0.339802305057]\n",
      "Intercept: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients/Weights: \" + str(logit_model.weights))\n",
    "print(\"Intercept: \" + str(logit_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " # Save and load model\n",
    "myModelPath=\"myModelPath\"\n",
    "logit_model.save(sc, myModelPath)\n",
    "sameModel = logit_model.load(sc, myModelPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
